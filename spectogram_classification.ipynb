{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook (and script) contains full code to my participation in Rainforest Connection Species Audio Detection competition\n",
    "\n",
    "https://www.kaggle.com/c/rfcx-species-audio-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%run kaggle_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all training data as mel-spectograms images\n",
    "* 1) 10 second random(or center) crop around particular label\n",
    "* 2) split 1 training file to 6 consequent 10 seconds chunks - this is for evaluation of valid files with same strategy as testing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = pd.read_csv(\"E:\\\\kaggle_data\\\\train\\\\_tp.csv\")\n",
    "\n",
    "path_train = \"E:\\\\kaggle_data\\\\train\"\n",
    "path_to_save_train = \"E:\\\\kaggle_data\\\\spectograms\\\\train_full\"\n",
    "path_to_save_valid = \"E:\\\\kaggle_data\\\\spectograms\\\\valid_full\"\n",
    "\n",
    "file_length = 10\n",
    "fmax = 20000\n",
    "\n",
    "for index, row in files.iterrows():\n",
    "    \n",
    "    file_to_load = f\"{files['recording_id'][index]}.flac\"\n",
    "    \n",
    "    data, sr = load_audio(os.path.join(path_train, file_to_load))\n",
    "    \n",
    "    # crreating of spectogram from labeled crop\n",
    "    spectogram = prepare_mel_spectogram(data,\n",
    "                        sampling_rate = sr,\n",
    "                        tmin = files['t_min'][index],\n",
    "                        tmax = files['t_max'][index],\n",
    "                        fmin = 90,\n",
    "                        fmax = fmax, \n",
    "                        crop = 'center'                \n",
    "                       )\n",
    "    \n",
    "    filename = f\"{files['species_id'][index]}_{files['recording_id'][index]}_{int(round(files['t_min'][index]*10,0))}.png\"\n",
    "    fullpath = os.path.join(path_to_save_train, filename)\n",
    "    cv2.imwrite(fullpath, spectogram)\n",
    "\n",
    "    \n",
    "    # creating of 6 spectograms from consequent 10-seconds chunks\n",
    "    for start in np.arange(0, 60, file_length):\n",
    "    \n",
    "        spectogram = prepare_mel_spectogram(data,\n",
    "                                            sampling_rate = sr,\n",
    "                                            tmin = start,\n",
    "                                            tmax = start + file_length,\n",
    "                                            fmin = 90,\n",
    "                                            fmax = fmax,\n",
    "                                            crop = 'center'\n",
    "                                           )\n",
    "        filename = f\"{file_to_load.split('.')[0]}_{start}.png\"       \n",
    "        fullpath = os.path.join(path_to_save_valid, filename)\n",
    "        cv2.imwrite(fullpath, spectogram)\n",
    "    \n",
    "    \n",
    "    #if index % 50 ==0:\n",
    "    #    print(f\"Proceeded {index} images out of {files.shape[0]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving testing set as 6x10s chunks mel-spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_length = 10 #split test file to file length seconds\n",
    "path_test = \"E:\\\\kaggle_data\\\\test\"\n",
    "test_files = os.listdir(path_test)\n",
    "path_to_save = \"E:\\\\kaggle_data\\\\spectograms\\\\test\"\n",
    "fmax = 20000\n",
    "\n",
    "counter = 0\n",
    "for file in test_files:  \n",
    "    data, sr = load_audio(os.path.join(path_test, file))\n",
    "    \n",
    "    for start in np.arange(0, 60, file_length):\n",
    "    \n",
    "        spectogram = prepare_mel_spectogram(data,\n",
    "                                            sampling_rate = sr,\n",
    "                                            tmin = start,\n",
    "                                            tmax = start + file_length,\n",
    "                                            fmin = 90,\n",
    "                                            fmax = fmax,\n",
    "                                            crop = 'center')\n",
    "        \n",
    "        filename = f\"{file.split('.')[0]}_{start}.png\"\n",
    "        \n",
    "        fullpath = os.path.join(path_to_save, filename)\n",
    "        cv2.imwrite(fullpath, spectogram)\n",
    "    \n",
    "    #if counter % 50 ==0:\n",
    "    #    print(f\"Proceeded {counter} images\") \n",
    "    #counter = counter+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise some mel-spectograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 3 mel-spectograms with label 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "path_train = \"E:\\\\kaggle_data\\\\spectograms\\\\train_full\"\n",
    "\n",
    "desired_index = 19   # images with this target should be plotted\n",
    "image_number = 3    # first n images with this target will be plotted\n",
    "\n",
    "print_images(path_train, desired_index, image_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 3 mel-spectograms with label 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"E:\\\\kaggle_data\\\\spectograms\\\\train_full\"\n",
    "\n",
    "desired_index = 10 # images with this target should be plotted\n",
    "image_number = 3    # first n images with this target will be plotted\n",
    "\n",
    "print_images(path_train, desired_index, image_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet training and evaluation phase definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setlr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "def lr_decay(optimizer, epoch):\n",
    "    if epoch%8==0:\n",
    "        new_lr = learning_rate / (10**(epoch//8))\n",
    "        optimizer = setlr(optimizer, new_lr)\n",
    "        print(f'Changed learning rate to {new_lr}')\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, curr_fold, num_epochs, change_lr, evaluate_fake = False):\n",
    "\n",
    "    # TRAIN PHASE\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    train_LWLRAP = []\n",
    "    valid_LWLRAP = []\n",
    "    \n",
    "    \n",
    "    files = pd.read_csv(\"E:\\\\kaggle_data\\\\train\\\\_tp.csv\")\n",
    "    labs = []\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0,24):\n",
    "        labs.append(f's{i}_true')\n",
    "        preds.append(f's{i}')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-----------------------------------')\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "\n",
    "        \n",
    "        model.train() \n",
    "        #Tells the model to use train mode\n",
    "        #Dropout layer behaves differently for train/eval phases\n",
    "        \n",
    "        actual_loss = 0\n",
    "        num_corrects = 0\n",
    "        \n",
    "        if change_lr:\n",
    "            optimizer = change_lr(optimizer, epoch)       \n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            # otherwise by calling loss.backward() gradient of parameters would be summed\n",
    "            \n",
    "            outputs = model(inputs) \n",
    "            \n",
    "            loss = criterion(outputs, labels)     \n",
    "            # creates graph of parameters, is connected to model throught outputs\n",
    "        \n",
    "            loss.backward()  #computes gradient of loss with respect to the parameters\n",
    "            optimizer.step() #updates models parameters\n",
    "            # it is possible to put optimizer.step and optimizer.zero(grad) out of batch for with slower conv\n",
    "            \n",
    "            actual_loss += loss.item() * inputs.size(0) #sum of losses for given batch\n",
    "            num_corrects += torch.sum(outputs.argmax(dim=1) == labels.argmax(dim=1)).item() \n",
    "\n",
    "        train_loss.append(actual_loss / len(train_data)) \n",
    "        train_acc.append(num_corrects / len(train_data))\n",
    "\n",
    "        print(f'train Loss: {round(train_loss[epoch],6)} Acc: {round(train_acc[epoch],6)} Num_corrects: {round(num_corrects,6)}/ {len(train_data)} ')\n",
    "        \n",
    "        \n",
    "        #VALIDATION_PHASE\n",
    "        \n",
    "        with torch.no_grad(): # is it necessary?\n",
    "\n",
    "            model.eval() \n",
    "            \n",
    "            \n",
    "            #computing LWLRAP everytime XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "            evaluated_data = score_and_extend(evaluate_path, model, list_train, files)\n",
    "            actual_LWLRAP= LWLRAP(torch.Tensor(evaluated_data[preds].to_numpy()),\n",
    "                            torch.Tensor(evaluated_data[labs].to_numpy()))    \n",
    "            train_LWLRAP.append(actual_LWLRAP)\n",
    "            print(f\"TRAIN LWLRAP:{actual_LWLRAP}\")\n",
    "            \n",
    "            evaluated_data = score_and_extend(evaluate_path, model, list_valid, files)\n",
    "            actual_LWLRAP= LWLRAP(torch.Tensor(evaluated_data[preds].to_numpy()),\n",
    "                            torch.Tensor(evaluated_data[labs].to_numpy()))    \n",
    "            valid_LWLRAP.append(actual_LWLRAP)\n",
    "            print(f\"VALID LWLRAP:{actual_LWLRAP}\")\n",
    "\n",
    "\n",
    "            #XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "            \n",
    "\n",
    "            if evaluate_fake == True:\n",
    "            \n",
    "                valid_actual_loss = 0\n",
    "                valid_num_corrects = 0\n",
    "        \n",
    "                for inputs, labels in valid_loader:\n",
    "                    inputs = inputs.to(device, dtype=torch.float)\n",
    "                    labels = labels.to(device, dtype=torch.float)\n",
    "            \n",
    "                    outputs = model(inputs)               \n",
    "                    loss = criterion(outputs, labels)\n",
    "            \n",
    "                    valid_actual_loss += loss.item() * inputs.size(0)\n",
    "                    valid_num_corrects += torch.sum(outputs.argmax(dim=1) == labels.argmax(dim=1)).item()\n",
    "            \n",
    "                valid_loss.append(valid_actual_loss / len(valid_data))\n",
    "                actual_acc = valid_num_corrects / len(valid_data)\n",
    "                valid_acc.append(valid_num_corrects / len(valid_data))\n",
    "\n",
    "                \n",
    "                print(f'valid Loss: {round(valid_loss[epoch],6)} Acc: {round(valid_acc[epoch],6)} Num_corrects: {round(valid_num_corrects,6)}/ {len(valid_data)} ')\n",
    "                    \n",
    "    torch.save(model, f'E:\\\\kaggle_data\\\\saved_models\\\\model_{curr_fold}')\n",
    "             \n",
    "    if evaluate_fake == True:\n",
    "        fig, axs = plt.subplots(2, figsize=(14,8))\n",
    "        axs[0].plot(train_loss)\n",
    "        axs[0].plot(valid_loss)\n",
    "        axs[0].set_title('blue = Train BCE, orange = Valid Bce')\n",
    "\n",
    "        axs[1].plot(train_LWLRAP)\n",
    "        axs[1].plot(valid_LWLRAP)\n",
    "        axs[1].set_title('Blue = Train LWLRAP, orange = Valid LWLRAP')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = pd.read_csv(\"E:\\\\kaggle_data\\\\train\\\\_tp.csv\")\n",
    "path_train = \"E:\\\\kaggle_data\\\\spectograms\\\\train_full\"\n",
    "evaluate_path = \"E:\\\\kaggle_data\\\\spectograms\\\\valid_full\"\n",
    "num_classes=24\n",
    "\n",
    "n_splits = 4\n",
    "cross_val = StratifiedKFold(n_splits= n_splits, shuffle=True, random_state = 10)\n",
    "\n",
    "valid_acc = []\n",
    "valid_lwlrap = []\n",
    "\n",
    "i = 0\n",
    "for train_indexes, valid_indexes in cross_val.split(files['recording_id'], files['species_id']):\n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "    print(f'Starting fold {i+1}')\n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "    \n",
    "    \n",
    "    list_train = files['recording_id'][train_indexes].unique().tolist()\n",
    "    list_valid = files['recording_id'][valid_indexes].unique().tolist()\n",
    "    list_valid = [x for x in list_valid if x not in list_train] # remove from valid recording ids that are in both train and valid\n",
    "    \n",
    "    train_data = torch_dataset(list_train, path_train, num_classes, phase = 'train') \n",
    "    \n",
    "    valid_data = torch_dataset(list_valid, path_train, num_classes, phase = 'valid')\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle = True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=16, shuffle = True)\n",
    "    print('Data loaded')\n",
    "    \n",
    "    model_ft = return_nn(num_classes, device)\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    optimizer = torch.optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_function = FocalLoss()\n",
    "    loss_function = loss_function.cuda()\n",
    "    \n",
    "    lwlrap = train_model(model_ft, loss_function, optimizer, i, 12, lr_decay ,evaluate_fake = True)\n",
    "    #valid_acc.append(acc)\n",
    "    valid_lwlrap.append(lwlrap)\n",
    "    i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORING OF TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_files = os.listdir(\"E:\\\\kaggle_data\\\\test\")\n",
    "test_files = [x.split('.')[0] for x in test_files]\n",
    "test_path = \"E:\\\\kaggle_data\\\\spectograms\\\\test\"\n",
    "\n",
    "result = score_images_test_set(test_path, test_files)\n",
    "result.to_csv('E:\\\\kaggle_data\\\\submission.csv', header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
